{"cells":[{"cell_type":"code","execution_count":null,"id":"80b3eca0","metadata":{},"outputs":[],"source":["!pip install prophet\n","from pyspark.sql import SparkSession, SQLContext, Row\n","from pyspark.sql.functions import hour, when, year, col, date_format, to_timestamp, round, coalesce, sum, lit, udf,  floor, datediff, weekofyear, min, count, avg\n","from pyspark.sql.functions import floor, datediff, lit, min, count, date_add, col, date_format, date_trunc\n","from pyspark.sql.functions import hour, when, col, date_format, year, to_timestamp, round, format_string, coalesce, sum, count, StringType, concat_ws, row_number,lit\n","from pyspark.sql.types import IntegerType\n","from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import IsolationForest\n","from sklearn.neighbors import LocalOutlierFactor\n","from sklearn.preprocessing import StandardScaler\n","from pyspark.ml.feature import StandardScaler\n","from pyspark.sql import Window\n","from prophet import Prophet\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.feature import StandardScaler\n","from google.cloud import bigquery\n","from pyspark.sql.functions import year, count\n","from google.cloud import storage\n","import os"]},{"cell_type":"code","execution_count":null,"id":"ef8ea55c","metadata":{},"outputs":[],"source":["class suppress_stdout_stderr(object):\n","    '''\n","    A context manager for doing a \"deep suppression\" of stdout and stderr in\n","    Python, i.e. will suppress all print, even if the print originates in a\n","    compiled C/Fortran sub-function.\n","       This will not suppress raised exceptions, since exceptions are printed\n","    to stderr just before a script exits, and after the context manager has\n","    exited (at least, I think that is why it lets exceptions through).\n","\n","    '''\n","    def __init__(self):\n","        # Open a pair of null files\n","        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n","        # Save the actual stdout (1) and stderr (2) file descriptors.\n","        self.save_fds = (os.dup(1), os.dup(2))\n","\n","    def __enter__(self):\n","        # Assign the null pointers to stdout and stderr.\n","        os.dup2(self.null_fds[0], 1)\n","        os.dup2(self.null_fds[1], 2)\n","\n","    def __exit__(self, *_):\n","        # Re-assign the real stdout/stderr back to (1) and (2)\n","        os.dup2(self.save_fds[0], 1)\n","        os.dup2(self.save_fds[1], 2)\n","        # Close the null files\n","        os.close(self.null_fds[0])\n","        os.close(self.null_fds[1])\n","        \n","        \n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"]},{"cell_type":"code","execution_count":null,"id":"5d93ef9d","metadata":{},"outputs":[],"source":["class NYCabDataProcessor:\n","    \"\"\"\n","    Class to read NYC taxi data in from big query, clean \n","    the data (using Isolation Forest and basic filtration \n","    steps), join it using a lookup table, aggregate \n","    individual trip data into a weekly basis, and forecast \n","    the future. Other class methods can be used to build \n","    networks based on various critera.\n","    \n","    Attributes\n","    ----------\n","    data : pyspark.sql.DataFrame\n","        Contains the 2019 and 2020 trip data after being read \n","        in from bigquery via read_data. Also contains cleaned\n","        data from data cleaning methods\n","\n","    anomalous_data : pyspark.sql.DataFrame\n","        Contains a subset of the trip data that is anomalous.\n","        Used to visualize anomalous trips\n","\n","    data_with_lookup: pyspark.sql.DataFrame\n","        Merges data class variable with externally provided \n","        lookup table. Allows connection of each zone with\n","        geographic data.\n","    \n","    agg_data: pyspark.sql.DataFrame\n","        In order to forecast and visualize data on a \n","        reasonable level, contains total trips, total tips,\n","        and other statistics aggregated on weekly basis\n","\n","    forcasted_data: pyspark.sql.DataFrame\n","        Contains data forcasted past March 15 as provided by\n","        class methods using the Prophet model\n","        \n","    mape: float\n","        Used to store the mean absolute percentage error (MAPE)\n","        of the forecasst created.\n","    \n","    final_prices: pyspark.sql.DataFrame\n","        Used to store a dataframe of final prices to be used by the dashboard\n","        \n","    map_data_weekly_with_forecast: pyspark.sql.DataFrame\n","        Used to store the combined aggregated cleaned data along with taxi lookup and forecasted data\n","    \n","    map_delta_data_weekly: pyspark.sql.DataFrame\n","        Used in the dashboard, the final output of this notebook. Formatting and column\n","        names changed to make visualization easier.\n","    \n","    Methods\n","    -------\n","    read_data (filename1, filename2):\n","        Calls read_bigquery_data twice to read in two tables.\n","        For the purposes of COVID-19 forcasting, it is suggested\n","        to read in the 2019 and 2020 trip data.\n","\n","    read_bigquery_data(table_name):\n","        Reads bigquery data in given by the path passed into the\n","        class method. Mainly used by read_data.\n","\n","    read_csv_through_path(gs_path):\n","        Reads a CSV at a file path provided into a spark DataFrame\n","\n","    report_metadata:\n","        Utility function to print the metadata of the data class \n","        variable: Count of rows and column names\n","\n","    upload_data_to_bigquery(project, bucket_name, dataset_id, table_name, df):\n","        Utility function to write final data back to BigQuery at the end\n","        of the data session. Can specify which class dataframe to write.\n","\n","    clean_data:\n","        Base function to do some basic data cleaning. Cast columns\n","        to DateTime, switch columns to doubles and floats for \n","        calculation accuracy/performance as necessary, \n","        and filter all negative trip distances.\n","\n","    anomaly_profile_flag(K=3):\n","        Flag data as anomalous using an isolation forest algorithm.\n","        K = 3 is used as a hyperparameter and can be changed by the \n","        user to find different results.\n","    \n","    join_data_and_lookup(taxi_lookup):\n","        Joins the base data class variable on the taxi_lookup data \n","        provided in a CSV in the project folder.\n","\n","    aggregate_data_weekly:\n","        Aggregates data on a weekly basis, from individual trips to\n","        all trips on a weekly basis. Takes the trip count sums, but\n","        also takes the min of other columns in the dataframe grouped\n","        on a weekly basis.\n","\n","    wmape(actual, forecast):\n","        Calculates the wmape on the forecasted data from the prophet \n","        model.\n","\n","    forecast_data_post_march_fifteen:\n","        Uses facebook prophet model and aggregated data provided \n","        by weekly model to forecast future data and place into\n","        class variable for forecated data\n","\n","    get_mape_for_forecast_data:\n","        Uses wmape function, base data, and forecasted data to find\n","        the MAPE for the forecast.\n","        \n","    create_network_data(taxi_lookup):\n","        Aggregate all of the data from the processed trip data and format it into a single dataframe. \n","        Will be used in combination with the forecasted data to create differentials and map visuals.\n","    \n","    combined_forecasted_Data(taxi_lookup):\n","        Combined the forecast data with the network data to produce a CSV ready for a dashboard,\n","        complete with names of zones and boroughs, as well as shape files.\n","\n","    update(self,filename1,filename2, taxi_lookup):\n","        Wrapper function for class methods that reads, cleans, \n","        forecasts, and finds error for the data for 2019\n","        and 2020 taxi trips on Google BigQuery.\n","\n","\n","    \"\"\"\n","\n","    # Initialize all class variables to None (described above)\n","    data = None\n","    anomalous_data = None\n","    data_with_lookup = None\n","    agg_data = None\n","    forecasted_data = None\n","    mape = None\n","    final_prices = None\n","    nyc_network_df = None\n","    map_weekly_data_with_forecast = None\n","    map_data_delta_weekly = None\n","    \n","    \n","\n","\n","    def __init__(self, project, bucket, appname='spark-anomaly-detection'):\n","        self.spark = SparkSession.builder.master('yarn').appName(appname).getOrCreate()\n","        self.project = project\n","        self.bucket = bucket\n","        \n","    def read_data(self, filename1 : str , filename2: str):\n","        \"\"\"Reads data from Google BigQuery using class methods from filenames.\n","\n","        Args:\n","            filename1 (str): First filename to read from\n","            filename2 (str): Second filename to read from\n","        \"\"\"\n","        data_2019 = self.read_bigquery_table(filename1)\n","        data_2020 = self.read_bigquery_table(filename2)\n","        # Join the two dataframes and store in class variable data\n","        self.data = data_2019.union(data_2020)\n","    \n","    def read_bigquery_table(self,table_name):\n","        \"\"\"Uses pyspark to read data in from a location called table_name.\n","\n","        Args:\n","            table_name (str): Location of the file to be read from BigQuery\n","\n","        Returns:\n","            pyspark.sql.DataFrame: Data in dataframe read in from BigQuery\n","        \"\"\"\n","        return self.spark.read.format('bigquery').option('table', table_name).load()\n","    \n","    def read_csv_through_path(self,gs_path):\n","        \"\"\"Reads data in from a csv and stores in spark DataFrame\n","\n","        Args:\n","            gs_path (str): Relative or absolute path to csv\n","\n","        Returns:\n","            pyspark.sql.DataFrame: Data from csv\n","        \"\"\"\n","        return self.spark.read.csv(gs_path, header=True, inferSchema=True)\n","\n","    \n","    def report_metadata(self):\n","        \"\"\"Reports some basic metadata about the data class variable\n","\n","        Returns:\n","            dict: Dictionary containing a count of rows and list of colum names\n","        \"\"\"\n","        column_names = [field.name for field in self.data.schema]\n","        return {'row_count': self.data.count(), 'column_names': column_names}\n","    \n","\n","    def upload_data_to_bigquery(self,project_id, dataset_id, table_name, df):\n","        \"\"\"Utility to upload data back to bigquery. Input a project id, \n","        a dataset id, and a table name.\n","\n","        Args:\n","            project_id (str): String containing the Google project ID for BigQuery\n","            dataset_id (str): The \"bucket\" or dataset to store the data in\n","            table_name (str): The specific name of the file/data to be stored\n","            df (pyspark.sql.DataFrame): The DataFrame containing the data you would\n","                like to write.\n","        \"\"\"\n","        df.write.csv(f\"gs://{dataset_id}/{table_name}\",header=True,mode='overwrite')\n","\n","    \n","    \n","    def clean_data(self):\n","        \"\"\"Function do to some basic data cleaning and filtering. Performs the \n","        cleaning on the data class variable and changes it to store the data\n","        \"\"\"\n","        # Change the datetime columns to datetime data types\n","        data_ = self.data.withColumn(\"pickup_datetime\", self.data[\"pickup_datetime\"].cast(\"timestamp\"))\n","        data_ = data_.withColumn(\"dropoff_datetime\", data_[\"dropoff_datetime\"].cast(\"timestamp\"))\n","        # Cast the trip distance to a double for precision reasons\n","        data_ = data_.withColumn(\"trip_distance\", data_[\"trip_distance\"].cast(\"double\"))\n","        # Find the trip duration in minutes by calculating using datetime objects\n","        data_ = data_.withColumn(\"trip_duration\", (col(\"dropoff_datetime\").cast(\"long\") - col(\"pickup_datetime\").cast(\"long\")) / 60)\n","        # Filter the data such that the trip distance should be >= 0\n","        data_ = data_.filter(col(\"trip_distance\") >= 0)\n","        # Filter the data such that the total amount should be >= 0\n","        data_ = data_.filter(col(\"total_amount\") >= 0)\n","        # Filter the data for NA and NV zones (cannot be visualized on map)\n","        data_ = data_.filter(col(\"pickup_location_id\") < 264)\n","        data_ = data_.filter(col(\"dropoff_location_id\") < 264)\n","        # Filter data where dropoff date is earlier than pickup date\n","        data_ = data_.filter(col(\"dropoff_datetime\") >= col(\"pickup_datetime\"))\n","        self.data = data_ #cleaned data\n","\n","\n","    def anomaly_profile_flag(self,K=3):\n","        \"\"\"Use isolation forest algorithm, as well as Chebyshev's theorem\n","        to identify anomalous trips with regards to trip distance and trip \n","        duration. Store the anomalous trip data in a separate DataFrame \n","        for visualization reasons. Also creates some information describing \n","        the reasons that data can be anomalous.\n","\n","        Args:\n","            K (int=3): The K to use for Chebyshev's theorem (the number of \n","            standard deviations to allow in trip distance and duration)\n","        \"\"\"\n","        # Select the columns to find if anomalous\n","        selected_columns = [\"trip_distance\", \"trip_duration\"]\n","        # Select the other columns to train the anomaly detection on\n","        data_sub = self.data.select(*selected_columns)\n","        # Case to double for precision reason\n","        data_sub = data_sub.withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"double\"))\n","        data_sub = data_sub.withColumn(\"trip_duration\", col(\"trip_duration\").cast(\"double\"))\n","\n","        # Initialize lists to store the results\n","        IF_anomaly_distance_results = list()\n","        IF_anomaly_duration_results = list()\n","        IF_normal_distance_results = list()\n","        IF_normal_duration_results = list()\n","\n","        # Bootstrap the data to train the isolation forest on\n","        for i in range(1, 15):\n","            # Define a fraction of the data to sample for bootstrapping (set to small for computational reasons)\n","            sample_fraction = 0.0005  \n","            sampled_data = data_sub.sample(fraction=sample_fraction)\n","            sampled_pandas_df = sampled_data.toPandas()\n","            # Create the isolation forest trip ditances and durations\n","            isolation_forest_trip_distance = IsolationForest(contamination=0.03, random_state=42, n_jobs=-1)\n","            isolation_forest_trip_duration = IsolationForest(contamination=0.03, random_state=42, n_jobs=-1)\n","            # Fit the models\n","            isolation_forest_trip_distance.fit(sampled_pandas_df[[\"trip_distance\"]])\n","            isolation_forest_trip_duration.fit(sampled_pandas_df[[\"trip_duration\"]])\n","\n","            anomaly_labels_trip_distance = isolation_forest_trip_distance.predict(sampled_pandas_df[[\"trip_distance\"]])\n","            anomaly_labels_trip_duration = isolation_forest_trip_duration.predict(sampled_pandas_df[[\"trip_duration\"]])\n","\n","            sampled_pandas_df['distance_anomaly'] = anomaly_labels_trip_distance\n","            sampled_pandas_df['duration_anomaly'] = anomaly_labels_trip_duration\n","\n","            # Find what makes trips anomalous in terms of distance and duration in terms of the other variables\n","            distance_anomaly_description = sampled_pandas_df[sampled_pandas_df['distance_anomaly'] == -1]['trip_distance'].describe()\n","            distance_normal_description = sampled_pandas_df[sampled_pandas_df['distance_anomaly'] == 1]['trip_distance'].describe()\n","            duration_anomaly_description = sampled_pandas_df[sampled_pandas_df['duration_anomaly'] == -1]['trip_duration'].describe()\n","            duration_normal_description = sampled_pandas_df[sampled_pandas_df['duration_anomaly'] == 1]['trip_duration'].describe()\n","\n","            # Store the results\n","            IF_anomaly_distance_results.append(distance_anomaly_description)\n","            IF_anomaly_duration_results.append(duration_anomaly_description)\n","            IF_normal_distance_results.append(distance_normal_description)\n","            IF_normal_duration_results.append(duration_normal_description)\n","\n","            # Find the mean and standard deviation of each characteristic\n","            distance_normal_mean = pd.DataFrame(IF_normal_distance_results).mean()['mean']\n","            distance_normal_std = pd.DataFrame(IF_normal_distance_results).mean()['std']\n","            duration_normal_mean = pd.DataFrame(IF_normal_duration_results).mean()['mean']\n","            duration_normal_std = pd.DataFrame(IF_normal_duration_results).mean()['std']\n","\n","            # Find the normal bounds as defined by K standard deviations from the mean\n","            lower_bound_distance_normal = distance_normal_mean - K * distance_normal_std\n","            upper_bound_distance_normal = distance_normal_mean + K * distance_normal_std\n","\n","            lower_bound_duration_normal = duration_normal_mean - K * duration_normal_std\n","            upper_bound_duration_normal = duration_normal_mean + K * duration_normal_std\n","            \n","        # Finally aggregate all of the anomalous data into a separate class variable\n","        data_sub_anomalous = self.data.withColumn(\"is_anomalous_distance\",\n","                                      when((col(\"trip_distance\") < lower_bound_distance_normal) | \n","                                           (col(\"trip_distance\") > upper_bound_distance_normal), lit(True))\n","                                      .otherwise(lit(False)))\n","        # Store column for anomalous duration\n","        data_sub_anomalous = data_sub_anomalous.withColumn(\"is_anomalous_duration\",\n","                                                          when((col(\"trip_duration\") < lower_bound_duration_normal) | \n","                                                               (col(\"trip_duration\") > upper_bound_duration_normal), lit(True))\n","                                                          .otherwise(lit(False)))\n","        # Store column for anomalous in any category\n","        data_sub_anomalous = data_sub_anomalous.withColumn(\"is_anomalous\",\n","                                                          (col(\"is_anomalous_distance\") | col(\"is_anomalous_duration\")))\n","        # Assign to class variable\n","        self.anomalous_data = data_sub_anomalous\n","        \n","        \n","    \n","    #Aggregation\n","    def join_data_and_lookup(self,taxi_lookup):\n","        \"\"\"Join the data on the zone lookup containign things like path declarations \n","        and other metadata (like zone name and borough)\n","\n","        Args:\n","            taxi_lookup (pyspark.sql.DataFrame): Table containing taxi lookup data\n","        \"\"\"\n","        #filter to relevant columns\n","        processed_df = self.anomalous_data.select(\"vendor_id\", \"pickup_datetime\", \"dropoff_datetime\", \"trip_distance\", \"payment_type\", \"tip_amount\",\"total_amount\", \"pickup_location_id\", \"dropoff_location_id\", \"trip_duration\", \"is_anomalous\", \"is_anomalous_distance\", \"is_anomalous_duration\")\n","        processed_df = processed_df.withColumn(\"year\", year(\"pickup_datetime\"))\n","        processed_df = processed_df.filter((col(\"year\") == 2019) | (col(\"year\") == 2020))\n","        processed_df = processed_df.withColumnRenamed(\"pickup_location_id\", \"LocationID\")\n","        joined_df = processed_df.join(taxi_lookup, taxi_lookup.zone_id == processed_df.LocationID, 'left')\n","        filtered_df = joined_df.filter(col(\"pickup_datetime\") >= \"2019-01-01\")\n","        sorted_df = filtered_df.orderBy(col(\"pickup_datetime\"))\n","    \n","        self.data_with_lookup = sorted_df\n","        \n","        \n","        \n","    \n","    def aggregate_data_weekly(self):\n","        \"\"\"Aggregate the data with lookup class variable to a weekly\n","        basis in order to forecast into the future. Assumes\n","        2019 and 2020 data, as the first sunday is hardcoded for now.\n","        Takes the count of the trips in each week, but takes the min\n","        of other columns. Could be updated in future for other \n","        visualizations.\n","\n","        \"\"\"\n","        first_sunday = \"2019-01-06\"\n","        filtered_df = self.data_with_lookup.filter(col(\"pickup_datetime\") >= first_sunday)\n","    \n","        agg_df = filtered_df.withColumn(\n","           \"days_since_first_sunday\",\n","           datediff(col(\"pickup_datetime\"), lit(first_sunday))\n","        )\n","\n","        # Calculate the number of weeks since the first Sunday in 2019\n","        agg_df = agg_df.withColumn(\n","           \"weeks_since_first_sunday\",\n","           floor((col(\"days_since_first_sunday\") + 1) / 7).cast(IntegerType())\n","        )\n","\n","        # Calculate the start and end dates of each week\n","        agg_df = agg_df.withColumn(\n","            \"week_start_date\",\n","            date_add(lit(first_sunday), (col(\"weeks_since_first_sunday\") * 7))\n","        ).withColumn(\n","            \"week_end_date\",\n","            date_add(lit(first_sunday), (col(\"weeks_since_first_sunday\") + 1) * 7 - 1)\n","        )\n","\n","        # Calculate the correct \"week\" column representing the start of each week (Sunday)\n","        agg_df = agg_df.withColumn(\n","            \"week\",\n","            date_format(date_trunc(\"week\", col(\"week_start_date\")), \"yyyy-MM-dd HH:mm:ss\")\n","        )\n","\n","        nyc_aggregated_df = agg_df.groupBy(\"week_start_date\", \"LocationID\").agg(\n","            count(\"*\").alias(\"trip_count\"),\n","            *[sum(col(col_name).cast(\"int\")).alias(col_name) for col_name in filtered_df.columns if col_name in [\"is_anomalous\", \"is_anomalous_distance\", \"is_anomalous_duration\"]],\n","            *[sum(col_name).alias(col_name) for col_name in filtered_df.columns if col_name not in [ \"week_start_date\", \"week_end_date\", \"zone_id\", \"is_anomalous\", \"is_anomalous_distance\", \"is_anomalous_duration\",\"zone_name\", \"borough\", \"zone_geom\", \"LocationID\", \"trip_count\"]]\n","        )\n","\n","        nyc_aggregated_df = nyc_aggregated_df.select(\n","            \"week_start_date\",\n","            \"trip_count\",\n","            *[col_name for col_name in nyc_aggregated_df.columns if col_name not in [\"week_start_date\", \"week_end_date\", \"Zone\", \"trip_count\"]]\n","        )\n","\n","        #rename week_start_date to week and verify all columns are in dataset \n","        nyc_aggregated_df = nyc_aggregated_df.withColumnRenamed(\"week_start_date\", \"week\")\n","        self.agg_data = nyc_aggregated_df\n","        \n","    def find_mean_prices(self):\n","        \"\"\"Finds the mean price to be used for economic forecasts\n","        \"\"\"\n","        merged_df = self.data_with_lookup\n","        merged_df = merged_df.withColumn(\"LocationID\", col(\"LocationID\").cast(IntegerType()))\n","\n","        before_date = '2020-03-15'\n","        filtered_df = merged_df.filter(col(\"is_anomalous\") == False)\n","\n","        before_df = filtered_df[filtered_df['pickup_datetime'] < before_date]\n","        after_df = filtered_df[filtered_df['pickup_datetime'] >= before_date]\n","        before_totals = before_df.groupby('Borough').agg(avg('total_amount').alias('Mean_Total_Amount_Before'))\n","        after_totals = after_df.groupby('Borough').agg(avg('total_amount').alias('Mean_Total_Amount_After'))\n","\n","        final_prices_df = before_totals.join(after_totals, on=\"Borough\")\n","        self.final_prices = final_prices_df\n","        \n","\n","    def wmape(self,actual, forecast):\n","        \"\"\"Calculate the MAPE of two pandas DataFrames (use\n","        Series for better robust code).\n","\n","        Args:\n","            actual (pd.DataFrame): DataFrame/Series containing actual values\n","            forecast (pd.DataFrame): DataFrame/Series containing the forecast values\n","\n","        Returns: \n","            float: WMAPE calculated by the method\n","        \"\"\"\n","        actual_np = np.array(actual)\n","        forecast_np = np.array(forecast)\n","\n","        if len(actual_np) != len(forecast_np):\n","            return None  \n","\n","        absolute_percentage_errors = np.abs((actual_np - forecast_np) / actual_np)\n","\n","        return np.sum(absolute_percentage_errors) / len(actual_np) * 100\n","\n","    \n","    def forecast_data_post_march_fifteen(self):\n","        \"\"\"Forecast the trip count for all NYC yellow taxis post March 15th \n","        on a weekly granularity. Uses Meta PROPHET model to perform this \n","        forecast. Creates the forecasted_data class variable at the end\n","        using the agg_data class variable.\n","        \"\"\"\n","        # Make the agg data into a pandas dataframe (easier for Prophet to work with)\n","        agg_df = self.agg_data.toPandas()\n","        agg_df['week'] = pd.to_datetime(agg_df['week'])\n","        unique_zones = agg_df.LocationID.drop_duplicates().to_list()\n","\n","        # Place for the results to go from the prediction\n","        results_df = pd.DataFrame(columns=['week', 'LocationID', 'forecast_qty', 'trip_count'])\n","\n","        # For each of the zones in the list of zones, train the model on 2019 data\n","        # and forecast into 2020.\n","        for zone in unique_zones:\n","\n","            # Find the data only for the given zone\n","            zone_data = agg_df[agg_df['LocationID'] == zone]\n","            # Adjust the columns names for the sake of the prophet model\n","            zone_data.rename(columns={'week': 'ds', 'trip_count': 'y'}, inplace=True)\n","                        \n","            # Check for and handle NaN values in the target variable\n","            zone_data = zone_data.dropna(subset=['y'])\n","\n","            # Check for and handle missing values in the 'ds' column\n","            zone_data = zone_data.dropna(subset=['ds'])\n","\n","            # Remove duplicates in the 'ds' column\n","            zone_data = zone_data.drop_duplicates(subset=['ds'])\n","\n","            # Define training period/forecast period\n","            train_start_date = '2019-01-01'\n","            train_end_date = '2020-03-08'\n","            forecast_start_date = '2020-03-09'\n","            forecast_end_date = '2020-12-31'\n","\n","            # Filter down the to the necessary data\n","            train_df = zone_data[(zone_data['ds'] >= train_start_date) & (zone_data['ds'] <= train_end_date)]\n","            train_df = train_df.sort_values(by='ds')\n","\n","            # Forecast the future data\n","            forecast_df = zone_data[(zone_data['ds'] >= forecast_start_date) & (zone_data['ds'] <= forecast_end_date)]\n","            forecast_df = forecast_df.sort_values(by='ds')\n","\n","            # Create a prophet model instance\n","            model = Prophet()\n","            \n","            if len(train_df) > 1:\n","                # Suppress model output\n","                with suppress_stdout_stderr():\n","                    model.fit(train_df)\n","\n","                # Create an empty dataframe for the forecast of the future\n","                future = pd.DataFrame(forecast_df['ds']).reset_index(drop=True)\n","\n","\n","                # Use the trained model to create the forecast results\n","                if len(future) > 1:\n","                    forecast = model.predict(future)\n","                    forecast['yhat'] = forecast['yhat'].round(0).astype(int)\n","\n","                    observed = zone_data[['ds', 'y']]\n","                    predicted = forecast[['ds', 'yhat']]\n","\n","                    result_df = observed.merge(predicted, on='ds')\n","                    result_df['LocationID'] = zone\n","                    result_df.rename(columns={'ds': 'week', 'y': 'trip_count', 'yhat': 'forecast_qty'}, inplace=True)\n","                    result_df = result_df.sort_values(by='week')\n","                    results_df = pd.concat([results_df, result_df], ignore_index=True)\n","                \n","        # Merge the final results into a the existing agg_df class variable and assign to forecasted data \n","        results_df = results_df.rename(columns={\"LocationID\": \"zone_id\"})\n","        agg_df = agg_df.rename(columns={\"LocationID\": \"zone_id\"})\n","        final_df = agg_df.merge(results_df, on=['week', 'zone_id'], how='left', suffixes=('_original', '_forecast'))\n","        self.forecasted_data = final_df\n","        \n","    \n","    \n","    def get_mape_for_forecast_data(self):\n","        \"\"\"Use the forecast data to get the MAPE for each part of the forecast.\n","        Also fits the model very similar to March 15 method, but generates\n","        the MAPE to figure out how well the model is fit to the training\n","        data.\n","        \"\"\"\n","        # Make agg_df spark dataframe into pandas dataframe for processing reasons\n","        agg_df = self.agg_data.toPandas()\n","        # Initialize variables to be used\n","        fbp_wmape = list()\n","        zones_ = list()\n","\n","        agg_df['week'] = pd.to_datetime(agg_df['week'])\n","        unique_zones = agg_df.LocationID.drop_duplicates().to_list()\n","        counter = 0\n","\n","        for zone in unique_zones:\n","\n","            # Some logging such that the user can verify progress is occurring.\n","            counter += 1\n","            if counter % 10 == 0:\n","                print(counter/len(unique_zones))\n","            zone_data = agg_df[agg_df['LocationID'] == zone]\n","            zone_data.rename(columns={'week': 'ds', 'trip_count': 'y'}, inplace=True)\n","            \n","            # Check for and handle NaN values in the target variable\n","            zone_data = zone_data.dropna(subset=['y'])\n","\n","            # Check for and handle missing values in the 'ds' column\n","            zone_data = zone_data.dropna(subset=['ds'])\n","\n","            # Remove duplicates in the 'ds' column\n","            zone_data = zone_data.drop_duplicates(subset=['ds'])\n","\n","            end_date = '2019-11-30'\n","\n","            train_df = zone_data[zone_data['ds'] <= end_date]\n","            test_df = zone_data[(zone_data['ds'] >= end_date) & (zone_data['ds'] <= '2020-01-01')]\n","\n","            model = Prophet()\n","\n","            if len(train_df) > 1:\n","                with suppress_stdout_stderr():\n","                            model.fit(train_df)\n","\n","                future = model.make_future_dataframe(periods=5, freq='W', include_history=True)\n","                future = future[future['ds'].dt.month == 12]\n","\n","                if len(future) > 1:\n","                    forecast = model.predict(future)\n","\n","                    observed = pd.DataFrame(test_df[test_df['ds'].dt.month == 12]['y']).reset_index(drop=True)\n","                    predicted = pd.DataFrame(forecast[forecast['ds'].dt.month == 12]['yhat']).reset_index(drop=True)\n","                    zones_.append(predicted)\n","                    wmape_ = self.wmape(observed, predicted)\n","                    fbp_wmape.append(wmape_)\n","        lst = [wmape for wmape in fbp_wmape if wmape is not None]\n","\n","        # Store the MAPE into the class variable\n","        self.mape = pd.DataFrame(lst).describe()[0]\n","        \n","    def create_network_data(self, taxi_lookup):\n","        processed_df = self.anomalous_data.select(\"vendor_id\", \"pickup_datetime\", \"dropoff_datetime\", \"trip_distance\", \"payment_type\", \"tip_amount\",\"total_amount\", \"pickup_location_id\", \"dropoff_location_id\", \"trip_duration\", \"is_anomalous\", \"is_anomalous_distance\", \"is_anomalous_duration\")\n","        processed_df = processed_df.withColumn(\"year\", year(\"pickup_datetime\"))\n","        processed_df = processed_df.filter((col(\"year\") == 2019) | (col(\"year\") == 2020))\n","        processed_df.groupBy(\"year\").agg(count(\"*\").alias(\"row_count\")).show()\n","        processed_df = processed_df.withColumnRenamed(\"pickup_location_id\", \"LocationID\")\n","        joined_df = processed_df.join(taxi_lookup, taxi_lookup.zone_id == processed_df.LocationID, 'left')\n","        filtered_df = joined_df.filter(col(\"pickup_datetime\") >= \"2019-01-01\")\n","        sorted_df = filtered_df.orderBy(col(\"pickup_datetime\"))\n","        first_sunday = \"2019-01-06\"\n","        filtered_df = sorted_df.filter(col(\"pickup_datetime\") >= first_sunday)\n","        agg_df = filtered_df.withColumn(\n","           \"days_since_first_sunday\",\n","           datediff(col(\"pickup_datetime\"), lit(first_sunday))\n","        )\n","\n","        agg_df = agg_df.withColumn(\n","           \"weeks_since_first_sunday\",\n","           floor((col(\"days_since_first_sunday\") + 1) / 7).cast(IntegerType())\n","        )\n","\n","        agg_df = agg_df.withColumn(\n","            \"week_start_date\",\n","            date_add(lit(first_sunday), (col(\"weeks_since_first_sunday\") * 7))\n","        ).withColumn(\n","            \"week_end_date\",\n","            date_add(lit(first_sunday), (col(\"weeks_since_first_sunday\") + 1) * 7 - 1)\n","        )\n","        agg_df = agg_df.withColumn(\n","            \"week\",\n","            date_format(date_trunc(\"week\", col(\"week_start_date\")), \"yyyy-MM-dd HH:mm:ss\")\n","        )\n","        agg_df_false = agg_df.where(agg_df.is_anomalous == False)\n","        nyc_aggregated_df = agg_df_false.groupBy(\"week_start_date\", agg_df[\"LocationID\"].alias(\"origin\"), agg_df[\"dropoff_location_id\"].alias(\"destination\")).agg(sum(\"total_amount\").alias(\"sum_total_amount\"), sum(\"tip_amount\").alias(\"sum_tip_amount\"), sum(\"trip_duration\").alias(\"sum_trip_duration\"), sum(\"trip_distance\").alias(\"sum_trip_distance\"), count(\"*\").alias(\"trip_count\"))\n","        nyc_aggregated_df = nyc_aggregated_df.join(taxi_lookup, nyc_aggregated_df.origin == taxi_lookup.zone_id, 'left')\n","        nyc_aggregated_df = nyc_aggregated_df.withColumnRenamed(\"zone_id\",\"zone_name_origin\")\n","        nyc_aggregated_df = nyc_aggregated_df.withColumnRenamed(\"Borough\",\"borough_origin\")\n","        nyc_aggregated_df = nyc_aggregated_df.withColumnRenamed(\"service_zone\",\"service_zone_origin\")\n","\n","        # join aggregated data set again with taxi_lookup based on destination/location ID this time\n","        nyc_aggregated_df = nyc_aggregated_df.join(taxi_lookup, nyc_aggregated_df.destination == taxi_lookup.zone_id, 'left')\n","        nyc_aggregated_df = nyc_aggregated_df.withColumnRenamed(\"zone_id\",\"zone_name_destination\")\n","        nyc_aggregated_df = nyc_aggregated_df.withColumnRenamed(\"Borough\",\"borough_destination\")\n","        nyc_aggregated_df = nyc_aggregated_df.withColumnRenamed(\"service_zone\",\"service_zone_destination\")\n","        self.nyc_network_df = nyc_aggregated_df\n","        \n","    def combined_forecasted_data(self, taxi_lookup):\n","        # make the starting data frame for map table w/ necessary columns, join network data with forecast data, rename cols\n","        forecasted_data_local = self.forecasted_data\n","        forecasted_data_local['forecast_qty'] = forecasted_data_local['forecast_qty'].fillna(0).astype(int)\n","        forecasted_data_local['trip_count_forecast'] = forecasted_data_local['trip_count_forecast'].fillna(0).astype(int)\n","        forecasted_data_local['zone_id'] = forecasted_data_local['zone_id'].fillna(0).astype(int)\n","        forecasted_data_local = self.spark.createDataFrame(self.forecasted_data)\n","        forecasted_data_local = forecasted_data_local.select(\"week\", \"zone_id\",\"trip_count_original\",\"forecast_qty\")\n","        self.nyc_network_df = self.nyc_network_df.join(forecasted_data_local, (self.nyc_network_df.week_start_date == forecasted_data_local.week) & (self.nyc_network_df.origin == forecasted_data_local.zone_id.cast(StringType())),\"left\")\n","        self.nyc_network_df = self.nyc_network_df.withColumnRenamed(\"forecast_qty\", \"forecasted_zone_origin_weight\")\n","        self.nyc_network_df = self.nyc_network_df.withColumnRenamed(\"trip_count_original\", \"actual_zone_origin_weight\")\n","        self.nyc_network_df = self.nyc_network_df.withColumnRenamed(\"trip_count\", \"weight\")\n","        \n","        # removing entries that have no origin or destination (won't work well with maps if you don't have two points to generate the path/line)\n","        self.nyc_network_df = self.nyc_network_df.filter(self.nyc_network_df.zone_name_origin.isNotNull())\n","        self.nyc_network_df = self.nyc_network_df.filter(self.nyc_network_df.zone_name_destination.isNotNull())\n","\n","        # creating path col by concating origin zone number with destination zone number\n","        self.nyc_network_df = self.nyc_network_df.withColumn(\"path\", concat_ws(\"_\",self.nyc_network_df[\"origin\"].cast(StringType()),self.nyc_network_df[\"destination\"].cast(StringType())))\n","        self.nyc_network_df = self.nyc_network_df.withColumn(\"year\", year(\"week_start_date\"))\n","        \n","        # creating origin and destination datasets and then will concat them (union wise) to make our dataset for map viz\n","\n","        data_all_o = self.nyc_network_df.select(\"week_start_date\",\"origin\",\"path\",\"weight\",\"actual_zone_origin_weight\",\"zone_name_origin\",\"borough_origin\",\"year\",\"sum_tip_amount\",\"sum_total_amount\",\"sum_trip_duration\",\"sum_trip_distance\",\"forecasted_zone_origin_weight\")\n","        data_all_o = data_all_o.withColumnRenamed(\"origin\",\"origin_destination\")\n","        data_all_o = data_all_o.withColumnRenamed(\"zone_name_origin\",\"zone_name\")\n","        data_all_o = data_all_o.withColumnRenamed(\"borough_origin\",\"borough\")\n","        data_all_o = data_all_o.withColumnRenamed(\"zone_geom_origin\",\"zone_geom\")\n","        data_all_o = data_all_o.withColumn(\"node_name\",format_string(\"origin\"))\n","\n","        data_all_d = self.nyc_network_df.select(\"week_start_date\",\"destination\",\"path\",\"weight\",\"actual_zone_origin_weight\",\"zone_name_destination\",\"borough_destination\",\"year\",\"sum_tip_amount\",\"sum_total_amount\",\"sum_trip_duration\",\"sum_trip_distance\")\n","        data_all_d = data_all_d.withColumnRenamed(\"destination\",\"origin_destination\")\n","        data_all_d = data_all_d.withColumnRenamed(\"zone_name_destination\",\"zone_name\")\n","        data_all_d = data_all_d.withColumnRenamed(\"borough_destination\",\"borough\")\n","        data_all_d = data_all_d.withColumnRenamed(\"zone_geom_destination\",\"zone_geom\")\n","        data_all_d = data_all_d.withColumn(\"node_name\",format_string(\"destination\"))\n","        \n","        union_edge_all = data_all_o.unionByName(data_all_d, allowMissingColumns=True)\n","        \n","        # create 2019 and 2020 datasets \n","        union_edge_2019 = union_edge_all.where(col(\"year\") == 2019)\n","        union_edge_2020 = union_edge_all.where(col(\"year\") == 2020)\n","\n","        # rename fields in 2020 data set so we can merge the datasets later on without issue\n","        union_2020_delta = union_edge_2020.select(\"week_start_date\",\"path\",\"weight\",\"forecasted_zone_origin_weight\",\"actual_zone_origin_weight\",\"node_name\")\n","        union_2020_delta = union_2020_delta.withColumnRenamed(\"week_start_date\",\"week_start_date_2020\")\n","        union_2020_delta = union_2020_delta.withColumnRenamed(\"path\",\"path_2020\")\n","        union_2020_delta = union_2020_delta.withColumnRenamed(\"forecasted_zone_origin_weight\",\"forecasted_zone_origin_weight_2020\")\n","        union_2020_delta = union_2020_delta.withColumnRenamed(\"actual_zone_origin_weight\",\"actual_zone_origin_weight_2020\")\n","        union_2020_delta = union_2020_delta.withColumnRenamed(\"node_name\",\"node_name_2020\")\n","        union_2020_delta = union_2020_delta.withColumnRenamed(\"weight\",\"weight_2020\")\n","        union_2019_delta = union_edge_2019\n","        \n","        year_2019_week_list = union_2019_delta.select(\"week_start_date\").distinct()\n","        year_2020_week_list = union_2020_delta.select(\"week_start_date_2020\").distinct()\n","        year_2020_week_list = year_2020_week_list.where(year_2020_week_list.week_start_date_2020 != \"2019-12-29\")\n","        \n","        w = Window().orderBy(\"week_start_date\")\n","        year_2019_week_list_key = year_2019_week_list.withColumn(\"week_number\", row_number().over(w))\n","        w = Window().orderBy(\"week_start_date_2020\")\n","        year_2020_week_list_key = year_2020_week_list.withColumn(\"week_number_2020\", row_number().over(w))\n","\n","        year_2019_week_list_key = year_2019_week_list_key.withColumnRenamed(\"week_start_date\",\"week_key_1\")\n","        year_2020_week_list_key = year_2020_week_list_key.withColumnRenamed(\"week_start_date_2020\",\"week_key_2\")\n","        \n","        union_2019_delta = union_2019_delta.join(year_2019_week_list_key, union_2019_delta.week_start_date == year_2019_week_list_key.week_key_1, \"left\")\n","        union_2020_delta = union_2020_delta.join(year_2020_week_list_key, union_2020_delta.week_start_date_2020 == year_2020_week_list_key.week_key_2, \"left\")\n","\n","        union_edge_data_delta = union_2019_delta.join(union_2020_delta, (union_2019_delta.week_number == union_2020_delta.week_number_2020) & (union_2019_delta.path == union_2020_delta.path_2020) & (union_2019_delta.node_name == union_2020_delta.node_name_2020) , \"left\")\n","        union_edge_data_delta = union_edge_data_delta.withColumn(\"delta_actual_zone_origin_weight\", (union_edge_data_delta.actual_zone_origin_weight - union_edge_data_delta.actual_zone_origin_weight_2020)) # the original origin zone weight in 2019 - 2020 (this came from the forecast table)\n","        union_edge_data_delta = union_edge_data_delta.withColumn(\"trip_weight_delta\", (union_edge_data_delta.weight - union_edge_data_delta.weight_2020)) # trip count in 2019 - trip count in 2020\n","        union_edge_data_delta = union_edge_data_delta.select(\"origin_destination\",\"path\",\"weight\",\"weight_2020\",\"trip_weight_delta\",\"actual_zone_origin_weight\",\"actual_zone_origin_weight_2020\",\"delta_actual_zone_origin_weight\",\"zone_name\",\"borough\",\"node_name\",\"week_number\",\"week_start_date\",\"week_start_date_2020\")\n","        \n","        self.map_weekly_data_with_forecast = union_edge_all\n","        self.map_data_delta_weekly = union_edge_data_delta\n","    \n","    def write_to_BQ(self, BQ_suffix):\n","        self.upload_data_to_bigquery(self.project, self.bucket, f\"final_aggregated_data{BQ_suffix}.csv\", self.agg_data)\n","        # Upload pandas dataframe to GCS\n","        bucket_name = self.bucket\n","        file_name = f\"final_forecasted_data{BQ_suffix}.csv\"\n","        self.forecasted_data.to_csv(file_name, index=False)\n","        client = storage.Client()\n","        bucket = client.get_bucket(bucket_name)\n","        blob = bucket.blob(file_name)\n","        blob.upload_from_filename(file_name)\n","        self.upload_data_to_bigquery(self.project, self.bucket, f\"final_prices{BQ_suffix}.csv\", self.final_prices)\n","        self.upload_data_to_bigquery(self.project, self.bucket, f\"map_weekly_data_with_forecast{BQ_suffix}.csv\", self.map_weekly_data_with_forecast)\n","        self.upload_data_to_bigquery(self.project, self.bucket, f\"map_data_delta_weekly{BQ_suffix}.csv\", self.map_data_delta_weekly)\n","\n","    def update(self,filename1,filename2, taxi_lookup,write_to_BQ=False, BQ_suffix=\"\"):\n","        \"\"\"Wrapper function or running the core methods of the class.\n","        Reads the data in, cleans it, finds anomalies, aggregates the data,\n","        trains a Prophet model and finds the quality of the fit, then uses\n","        the trained model to forecast model for 2020 once the COVID-19\n","        pandemic began.\n","\n","        Args:\n","            filename1 (str): Location of the first file on BigQuery (2019)\n","            filename2 (str): Location of the second file on BigQuery (2020)\n","            taxi_lookup (pyspark.sql.DataFrame): PySpark DataFrame containing the lookup data\n","\n","        Returns:\n","            pyspark.sql.DataFrame: The forecasted data\n","            float: The MAPE of the model used to forecast the data\n","        \"\"\"\n","        print(\"Reading Data\")\n","        self.read_data(filename1, filename2) #gives self.data\n","        print(\"Cleaning Data\")\n","        self.clean_data()\n","        print(\"Filtering anomalous data\")\n","        self.anomaly_profile_flag(K=3) #changes self.data\n","        print(\"Joining with taxi lookup data\")\n","        self.join_data_and_lookup(taxi_lookup) #changes self.data\n","        print(\"Finding mean prices\")\n","        self.find_mean_prices() # changes self.final_prices\n","        print(\"Aggregating to weekly level\")\n","        self.aggregate_data_weekly() #gives self.agg_data\n","        print(\"Beginning forecast\")\n","        self.get_mape_for_forecast_data()#get mape\n","        self.forecast_data_post_march_fifteen()\n","        print(\"Creating network data\")\n","        self.create_network_data(taxi_lookup)\n","        print(\"Designing network differential\")\n","        self.combined_forecasted_data(taxi_lookup)\n","        if write_to_BQ == True:\n","            print(\"Writing data\")\n","            self.write_to_BQ(BQ_suffix) # Write the data to the project\n","        return self.forecasted_data, self.mape"]},{"cell_type":"code","execution_count":null,"id":"fb460593","metadata":{"scrolled":false},"outputs":[],"source":["# Some code to use the class defined above\n","if __name__ == \"__main__\":\n","    # Set the options for this run (users change)\n","    google_project_name = 'test-project-406500'\n","    google_storage_bucket = 'example_bucket_143234'\n","    bigquery_suffix = \"_test\"\n","    # Create class object\n","    nyc_processor = NYCabDataProcessor(google_project_name,google_storage_bucket)\n","\n","    # Read in taxi lookup data\n","    bigquery_path = 'bigquery-public-data.new_york_taxi_trips.taxi_zone_geom'\n","    taxi_lookup = nyc_processor.read_bigquery_table(bigquery_path)\n","    \n","    # Location of the bigquery data\n","    filename1 = 'bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2019'\n","    filename2 = 'bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2020'\n","    # Run all of the data and store the results\n","    #final_df,mape = nyc_processor.update(filename1,filename2, taxi_lookup)\n","    final_df,mape = nyc_processor.update(filename1,filename2, taxi_lookup, write_to_BQ=True, BQ_suffix=bigquery_suffix)\n","    # Store the resulting data\n","    print('MAPE : ',mape)\n"]},{"cell_type":"code","execution_count":null,"id":"0cdfa536","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"186d2d50","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}